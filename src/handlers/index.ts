import { APIGatewayProxyHandlerV2 } from 'aws-lambda';

import { sha256, getObject, putObject } from '../lib/s3';
import { firecrawlScrape } from '../lib/firecrawl';
import { filterHomepageLinks, analyseImages } from '../lib/openai';  // â† added analyseImages
import { gptExtractImages } from '../lib/gpt-image-extract';
import { parseImages, RawImage } from '../lib/html-images';
import { dedupeImages, filterImages, hasValidFormat, uploadAllImagesToS3, filterS3ImagesByDimension } from '../lib/image-hash';

export const handler: APIGatewayProxyHandlerV2 = async (event: any) => {
  try {
    /* ---------- validation ---------- */
    if (!event.body) throw new Error('body missing');
    console.log('Debug: Raw event.body:', event.body);
    const { url, force_refresh = false } = JSON.parse(event.body);
    if (!url) throw new Error('url missing');
    const u = new URL(url);
    if (!/^https?:$/.test(u.protocol)) throw new Error('url must start with http/https');
    
    console.log('Debug: force_refresh parameter:', force_refresh);

    // Debug: Check if API keys are set (masked for security)
    const firecrawlKey = process.env.FIRECRAWL_API_KEY;
    const openaiKey = process.env.OPENAI_API_KEY;
    console.log('Debug: Firecrawl API key present:', !!firecrawlKey);
    console.log('Debug: OpenAI API key present:', !!openaiKey);
    console.log('Debug: Firecrawl key starts with:', firecrawlKey?.substring(0, 10) + '...');
    console.log('Debug: Deployment version:', process.env.DEPLOYMENT_VERSION);

    /* ---------- STEP 1 â€“ Firecrawl homepage (cached) ---------- */
    console.log('Step 1: Starting Firecrawl scrape for', url);
    const key = `${sha256(url)}/homepage.json`;
    let homepage = await getObject<any>(key);
    
    console.log('Step 1: Cache check - homepage exists:', !!homepage, 'force_refresh:', force_refresh);
    if (!homepage || force_refresh) {
      console.log('Step 1: No cached data or force refresh, scraping fresh');
      homepage = await firecrawlScrape(url, {
        onlyMainContent: false,
        formats: ['links', 'rawHtml'],
        maxAge: 0,            // â‡¦ disable read-cache
        storeInCache: false,  // â‡¦ don't write either
      });
      // Store with 24h TTL (86400 seconds)
      await putObject(key, homepage, 86400);
    } else {
      console.log('Step 1: Using cached data');
    }

    /* ---------- STEP 2 â€“ GPT-4.1 link filter ---------- */
    const homepageLinks: string[] = homepage.links ?? [];

    // Count how many links Firecrawl gave us
    console.info('Step 1: Firecrawl link count:', homepageLinks.length);          // ðŸž

    // Call GPT to keep only product-image pages
    const gptRaw = await filterHomepageLinks(homepageLinks);                      // returns string[]

    // Safeguard: if GPT returned a string instead of JSON, log it
    if (!Array.isArray(gptRaw)) {
      console.warn('Step 2: GPT output was not an array:', gptRaw);               // ðŸž
    }

    // Slice to top-1 and log
    const keptLinks = (Array.isArray(gptRaw) ? gptRaw : []).slice(0, 1);
    console.info('Step 2: Kept links:', keptLinks);                               // ðŸž

    /* ---------- STEP 3 â€“ Firecrawl top-1 page ---------- */
    console.log('Step 3: Scraping top pages');
    const pages = await Promise.all(
      keptLinks.map(async (link) => {
        const page = await firecrawlScrape(link, {
          onlyMainContent: true,
          formats: ['rawHtml'],
          maxAge: 0,
          storeInCache: false,
        });
        return { link, rawHTML: page.rawHTML ?? '' };
      })
    );
    console.log('Step 3: Scraped', pages.length, 'pages');

    /* ---------- STEP 4 â€“ harvest & dedupe images ---------- */
    let imgs = parseImages(homepage.rawHtml ?? '', url);
    pages.forEach((p) => (imgs = imgs.concat(parseImages(p.rawHTML, p.link))));
    console.info('Step 4a: regex parser found', imgs.length, 'images');

    if (imgs.length === 0) {
      // ðŸ§  FALLBACK: use GPT-4o-mini to pull URLs from raw HTML
      console.info('Step 4b: invoking GPT fallback');
      const htmlAll =
        (homepage.rawHtml ?? '') + pages.map((p) => p.rawHTML).join('\n');
      const gptUrls = await gptExtractImages(htmlAll, url);     // â† text-only call
      imgs = gptUrls.map((u: string) => ({ url: u, landingPage: url }));
      console.info('Step 4b: GPT returned', gptUrls.length, 'URLs');
    }

    const uniqueImgs = await dedupeImages(imgs);   // HEAD + pHash + Content-Length filter
    console.info('Step 4c: unique images after dedupe:', uniqueImgs.length);
    console.info('Step 4c: unique image URLs:', uniqueImgs.map(img => img.url));

    // Upload all images to S3 (AVIF to WebP, others as-is)
    const bucket = process.env.S3_BUCKET!;
    const s3Imgs = await uploadAllImagesToS3(uniqueImgs, bucket);
    console.info('Step 4d: images after S3 upload:', s3Imgs.length);
    console.info('Step 4d: S3 image URLs:', s3Imgs.map((img: RawImage & { hash: string }) => img.url));

    // Filter S3 images by dimension (at least one dimension >= 300px), limit to 5
    const filteredImgs = await filterS3ImagesByDimension(s3Imgs);
    console.info('Step 4e: images after dimension filter:', filteredImgs.length);
    console.info('Step 4e: filtered image URLs:', filteredImgs.map((img: RawImage & { hash: string }) => img.url));

    const limitedImgs = filteredImgs.slice(0, 5); // hard cap for test
    console.info('Step 4: limited image URLs:', limitedImgs.map((img: RawImage & { hash: string }) => img.url));
    console.log('Step 4: Found', limitedImgs.length, 'unique images');

    /* ---------- STEP 5 â€“ GPT-o4-mini analysis (jpeg/png/webp) ---------- */

    // 1 Â· filter to the formats we care about
    const eligible = limitedImgs.filter((img: RawImage & { hash: string }) => {
      // Only allow webp, jpeg, jpg, png
      return /\.(jpe?g|png|webp)(\?|$)/i.test(img.url);
    });

    let analysed: Awaited<ReturnType<typeof analyseImages>> = [];

    if (eligible.length) {
      // 2 Â· send at most five images for enrichment
      const sendToAI = eligible.slice(0, 5);
      console.info('Step 5: eligible images after format check:', eligible.length);
      console.info('Step 5: sending image URLs:', sendToAI.map((img: RawImage & { hash: string }) => img.url));
      console.info(
        `Step 5: sending ${sendToAI.length} of ${eligible.length} eligible images to AI`
      );
      analysed = await analyseImages(sendToAI);
    } else {
      console.warn('Step 5: no jpeg/png/webp images â€“ skipping AI step');
    }

    /* canonicalise â†’ drop query-string + lowercase extension */
    const canon = (u?: string) => (u ? u.split('?')[0] : '');

    /* build lookup by canonical URL */
    const aiByUrl = new Map(
      analysed
        .filter((a) => a.url)              // â¬… skip entries without url
        .map((a) => [canon(a.url), a])
    );

    const imagesFinal = limitedImgs.map((raw: RawImage & { hash: string }) => {
      const ai = aiByUrl.get(canon(raw.url));
      return {
        url: raw.url,
        alt: ai?.alt ?? raw.alt ?? '',
        landing_page: raw.landingPage,
        hash: raw.hash,
      };
    });

    console.log('Final: Returning', imagesFinal.length, 'images');
    /* ---------- FINAL response ---------- */
    return {
      statusCode: 200,
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        source_url: url,
        generated_at: new Date().toISOString(),
        images: imagesFinal,
      }),
    };
  } catch (err: any) {
    console.error('Lambda error:', err);
    return {
      statusCode: 400,
      body: JSON.stringify({ 
        error: err.message ?? 'unknown error',
        details: err.response?.data || err.stack || 'No additional details'
      }),
    };
  }
};

